{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gymnasium as gym\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "import xgboost as xgb\n",
    "import matplotlib.animation as animation\n",
    "from typing import Dict, Any\n",
    "from copy import deepcopy\n",
    "import random \n",
    "from sklearn.base import BaseEstimator\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.distributions.categorical import Categorical\n",
    "from sklearn.utils import gen_batches\n",
    "from collections import deque\n",
    "import pickle\n",
    "from torch.optim import Adam\n",
    "from copy import deepcopy\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import the BaseAgent class\n",
    "from src.agents.base_agent import BaseAgent\n",
    "from initial_windfields import get_initial_windfield, INITIAL_WINDFIELDS\n",
    "from src.env_sailing import SailingEnv\n",
    "from src.test_agent_validity import validate_agent, load_agent_class\n",
    "from src.evaluation import evaluate_agent, visualize_trajectory\n",
    "from src.utils.agent_utils import save_qdn_agent\n",
    "\n",
    "# Environment parameters\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "n_actions = env.action_space.n\n",
    "d_s = 2054"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the playing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_animation(imgs):\n",
    "  \"\"\"\n",
    "  Makes an animation from a list of images\n",
    "  Parameters\n",
    "  ----------\n",
    "  imgs: list of (height, width, 3) np arrays\n",
    "    list of images\n",
    "  Return\n",
    "  -------\n",
    "  ani: animation\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots()\n",
    "  draw = []\n",
    "  for i in range(len(imgs)):\n",
    "    draw_i = ax.imshow(imgs[i])\n",
    "    if i == 0:\n",
    "      ax.imshow(imgs[0]) # Show an initial one first\n",
    "    draw.append([draw_i])\n",
    "  plt.close()\n",
    "  ani = animation.ArtistAnimation(fig, draw, interval=200, blit=True,\n",
    "                              repeat=False)\n",
    "  return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_policy(env, pi, horizon=200, capture_rate=1):\n",
    "  s, _ = env.reset()\n",
    "  a = pi(s)\n",
    "  imgs = []\n",
    "  imgs.append(env.render())\n",
    "  for tt in range(horizon):\n",
    "    s, rew, term, trunc, _ = env.step(a)\n",
    "    a = pi(s)\n",
    "    if tt % capture_rate == 0:\n",
    "      imgs.append(env.render())\n",
    "    if term or trunc:\n",
    "      break\n",
    "  return make_animation(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_1\n",
      "{'wind_init_params': {'base_speed': 3.0, 'base_direction': (-0.8, -0.2), 'pattern_scale': 32, 'pattern_strength': 0.3, 'strength_variation': 0.4, 'noise': 0.1}, 'wind_evol_params': {'wind_change_prob': 1.0, 'pattern_scale': 128, 'perturbation_angle_amplitude': 0.1, 'perturbation_strength_amplitude': 0.1, 'rotation_bias': 0.02, 'bias_strength': 1.0}, 'env_params': {'wind_grid_density': 25, 'wind_arrow_scale': 80, 'render_mode': 'rgb_array'}}\n",
      "training_2\n",
      "{'wind_init_params': {'base_speed': 3.0, 'base_direction': (-0.2, 0.8), 'pattern_scale': 128, 'pattern_strength': 0.6, 'strength_variation': 0.3, 'noise': 0.1}, 'wind_evol_params': {'wind_change_prob': 1.0, 'pattern_scale': 128, 'perturbation_angle_amplitude': 0.1, 'perturbation_strength_amplitude': 0.1, 'rotation_bias': 0.02, 'bias_strength': 1.0}, 'env_params': {'wind_grid_density': 25, 'wind_arrow_scale': 80, 'render_mode': 'rgb_array'}}\n",
      "training_3\n",
      "{'wind_init_params': {'base_speed': 3.0, 'base_direction': (0.2, -0.8), 'pattern_scale': 32, 'pattern_strength': 0.4, 'strength_variation': 0.2, 'noise': 0.1}, 'wind_evol_params': {'wind_change_prob': 1.0, 'pattern_scale': 128, 'perturbation_angle_amplitude': 0.1, 'perturbation_strength_amplitude': 0.1, 'rotation_bias': 0.02, 'bias_strength': 1.0}, 'env_params': {'wind_grid_density': 25, 'wind_arrow_scale': 80, 'render_mode': 'rgb_array'}}\n",
      "simple_static\n",
      "{'wind_init_params': {'base_speed': 3.0, 'base_direction': (-0.7, -0.7), 'pattern_scale': 32, 'pattern_strength': 0.1, 'strength_variation': 0.1, 'noise': 0.05}, 'wind_evol_params': {'wind_change_prob': 0.0, 'pattern_scale': 128, 'perturbation_angle_amplitude': 0.0, 'perturbation_strength_amplitude': 0.0, 'rotation_bias': 0.0, 'bias_strength': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "for initial_windfield_name, initial_windfield in INITIAL_WINDFIELDS.items():\n",
    "    print(initial_windfield_name)\n",
    "    print(initial_windfield)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitted Q iteration\n",
    "\n",
    ">Implement fitted Q iterations with random forest using uniform exploration for $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect a dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [33:55:12<00:00, 1221.13s/it]  \n"
     ]
    }
   ],
   "source": [
    "class FQI(BaseAgent):\n",
    "    \"\"\" FQI agent\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_s = 2054\n",
    "        self.gamma = 0.99 \n",
    "        self.n_iterations = 100\n",
    "        self.n_actions = 9\n",
    "        self.epsilon = 0.1\n",
    "        self.model = RandomForestRegressor()\n",
    "        self.pi = None\n",
    "        self.data = None\n",
    "\n",
    "    def collect_dataset(self, n=10000):\n",
    "        pi = lambda x: np.random.randint(0, self.n_actions)\n",
    "        data = []\n",
    "        for initial_windfield_name, initial_windfield in INITIAL_WINDFIELDS.items():\n",
    "            env = SailingEnv(**get_initial_windfield(initial_windfield_name))\n",
    "            s0, _ = env.reset()\n",
    "            s = s0.copy()\n",
    "            n_actions = env.action_space.n\n",
    "\n",
    "            for i in range(n//4):\n",
    "                a = pi(s)\n",
    "                s2, r, done, trunc, _ = env.step(a)\n",
    "                data.append(s.copy().tolist() + [a, r, done] + s2.copy().tolist())\n",
    "                if done or trunc:\n",
    "                    s, _ = env.reset()\n",
    "                else:\n",
    "                    s = s2.copy()\n",
    "        return np.array(data)\n",
    "        \n",
    "    \n",
    "    def act(self, observation: np.ndarray) -> int:\n",
    "        if self.pi is None:\n",
    "            print(\"The Agent has not been trained\")\n",
    "        else:\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                return np.random.randint(self.n_actions)\n",
    "            return int(self.pi(observation))\n",
    "        \n",
    "    def trained_policy(self, observation):\n",
    "        values = []\n",
    "        for a in range(self.n_actions):\n",
    "            X = np.hstack((observation, [a])).reshape(1, -1)\n",
    "            values.append(self.model.predict(X)[0])\n",
    "        return np.argmax(values)\n",
    "    \n",
    "    def trainFQI(self):\n",
    "        # Use the data collected before\n",
    "        self.data = self.collect_dataset()\n",
    "        n = len(self.data)\n",
    "        states, actions, rewards, dones, next_states = self.data[:, :2054], self.data[:, 2054], self.data[:, 2054+1], self.data[:, 2054+2], self.data[:, 2054+3:]\n",
    "        X, Y = self.data[:, :2054+1], self.data[:, 2054+1]\n",
    "        self.model.fit(X, Y)\n",
    "\n",
    "        for _ in tqdm(range(self.n_iterations)):\n",
    "            Qmax = np.max(\n",
    "                            [\n",
    "                            self.model.predict(np.column_stack([\n",
    "                                            self.data[:, self.d_s + 3:],\n",
    "                                            np.ones(n).reshape(-1, 1) * a\n",
    "                                            ]))\n",
    "                                for a in range(self.n_actions)\n",
    "                            ],axis=0)\n",
    "            Y = self.data[:, self.d_s + 1] + self.gamma * (1 - dones) * Qmax\n",
    "            self.model.fit(X, Y)\n",
    "        \n",
    "        self.pi = lambda s: self.trained_policy(s)\n",
    "        self.save(path=\"models/FQI_trained.pkl\")\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the agent.\"\"\"\n",
    "        pass  # Nothing to reset in this simple agent\n",
    "    \n",
    "    def seed(self, seed: int = None) -> None:\n",
    "        \"\"\"Set the random seed.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Save the model and policy to a file.\"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'model': self.model,\n",
    "                'pi': self.pi\n",
    "            }, f)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Load the model and policy from a file.\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.model = data['model']\n",
    "            self.pi = data['pi']\n",
    "\n",
    "\n",
    "agent = FQI()\n",
    "agent.trainFQI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating agent on 4 training initial windfields...\n",
      "\n",
      "Initial windfield: simple_static\n",
      "  Success Rate: 0.00%\n",
      "  Mean Reward: 0.00\n",
      "  Mean Steps: 200.0\n",
      "\n",
      "Initial windfield: training_1\n",
      "  Success Rate: 20.00%\n",
      "  Mean Reward: 7.70\n",
      "  Mean Steps: 179.2\n",
      "\n",
      "Initial windfield: training_2\n",
      "  Success Rate: 0.00%\n",
      "  Mean Reward: 0.00\n",
      "  Mean Steps: 200.0\n",
      "\n",
      "Initial windfield: training_3\n",
      "  Success Rate: 20.00%\n",
      "  Mean Reward: 3.12\n",
      "  Mean Steps: 197.2\n",
      "\n",
      "==================================================\n",
      "OVERALL SUCCESS RATE: 10.00%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Choose which training initial windfields to evaluate on\n",
    "TRAINING_INITIAL_WINDFIELDS = [\"simple_static\", \"training_1\", \"training_2\", \"training_3\"]\n",
    "\n",
    "# Evaluation parameters for all initial windfields\n",
    "ALL_SEEDS = [42, 43, 44, 45, 46]  # Seeds to use for all evaluations\n",
    "ALL_MAX_HORIZON = 200             # Maximum steps per episode\n",
    "\n",
    "# Only run if the agent was successfully loaded\n",
    "if 'agent' in locals():\n",
    "    # Store results for each initial windfield\n",
    "    all_results = {}\n",
    "    \n",
    "    print(f\"Evaluating agent on {len(TRAINING_INITIAL_WINDFIELDS)} training initial windfields...\")\n",
    "    \n",
    "    # Evaluate on each initial windfield\n",
    "    for initial_windfield_name in TRAINING_INITIAL_WINDFIELDS:\n",
    "        print(f\"\\nInitial windfield: {initial_windfield_name}\")\n",
    "        \n",
    "        # Get the initial windfield\n",
    "        initial_windfield = get_initial_windfield(initial_windfield_name)\n",
    "        \n",
    "        # Run the evaluation\n",
    "        results = evaluate_agent(\n",
    "            agent=agent,\n",
    "            initial_windfield=initial_windfield,\n",
    "            seeds=ALL_SEEDS,\n",
    "            max_horizon=ALL_MAX_HORIZON,\n",
    "            verbose=False,  # Less verbose for multiple evaluations\n",
    "            render=False,\n",
    "            full_trajectory=False\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_results[initial_windfield_name] = results\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"  Success Rate: {results['success_rate']:.2%}\")\n",
    "        print(f\"  Mean Reward: {results['mean_reward']:.2f}\")\n",
    "        print(f\"  Mean Steps: {results['mean_steps']:.1f}\")\n",
    "    \n",
    "    # Print overall performance\n",
    "    total_success = sum(r['success_rate'] for r in all_results.values()) / len(all_results)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"OVERALL SUCCESS RATE: {total_success:.2%}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 295/10000 [00:42<1:11:40,  2.26it/s]"
     ]
    }
   ],
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 2032)\n",
    "        self.fc2 = nn.Linear(2032, 2032)\n",
    "        self.fc3 = nn.Linear(2032, 1016)\n",
    "        self.fc4 = nn.Linear(1016, 508)\n",
    "        self.fc5 = nn.Linear(508, 128)\n",
    "        self.fc6 = nn.Linear(128, output_dim)\n",
    "        self.dropout = nn.Dropout(p=0.2)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)  \n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.dropout(x)  \n",
    "        return self.fc6(x)\n",
    "\n",
    "class DQNAgent(BaseAgent):\n",
    "    \"\"\" DQN Agent with custom SailingFeatureExtractor \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.d_s = 2054  # Dimension de l'état\n",
    "        self.n_actions = 9  # Nombre d'actions\n",
    "        self.pi = lambda x: np.random.randint(0, self.n_actions)\n",
    "        self.PI = None\n",
    "        self.capacity = 2000\n",
    "        self.batch_size = 200  \n",
    "        self.eps = 0.2  # Valeur initiale de epsilon pour l'exploration\n",
    "        self.gamma = 0.99  \n",
    "        self.C = 20\n",
    "        self.n_iterations = 10000\n",
    "        self.nb_gradient_steps = 5\n",
    "        self.target_update_interval = 5  \n",
    "        self.tau = 0.005  # Taux de mise à jour douce\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "        # Initialisation du modèle Q-Network\n",
    "        self.q_network = DQNNetwork(self.d_s, self.n_actions).to(self.device)\n",
    "        self.target_network = deepcopy(self.q_network).to(self.device)  \n",
    "        self.optimizer = Adam(self.q_network.parameters(), lr=0.01)\n",
    "        \n",
    "        # Buffer de replay\n",
    "        self.buffer = deque(maxlen=self.capacity)\n",
    "        self.memory = []\n",
    "        self.criterion = torch.nn.SmoothL1Loss()  # Fonction de perte (SmoothL1Loss pour DQN)\n",
    "        self.env = SailingEnv(**get_initial_windfield(\"training_1\"))\n",
    "        self.reset()\n",
    "        self.episode = 0\n",
    "\n",
    "    def trainDQN(self, path):\n",
    "        epsilon = self.eps\n",
    "        epsilon_warmup = 1000  # Épisodes de warmup avant que epsilon commence à diminuer\n",
    "\n",
    "        for name, wf in INITIAL_WINDFIELDS.items():\n",
    "            self.env = SailingEnv(**get_initial_windfield(name))\n",
    "            state, _ = self.env.reset()\n",
    "\n",
    "            for t in tqdm(range(self.n_iterations)):\n",
    "                # Exploration plus forte au début\n",
    "                if t < epsilon_warmup:\n",
    "                    epsilon = 1.0  # Exploration totale au début\n",
    "                else:\n",
    "                    epsilon = max(self.epsilon_min, epsilon * self.epsilon_decay)\n",
    "\n",
    "                # Choisir l'action selon la politique ε-greedy\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = np.random.choice(self.n_actions)  # Exploration\n",
    "                else:\n",
    "                    action = self.greedy_action(state)  # Exploitation\n",
    "\n",
    "                # Passer à l'état suivant dans l'environnement\n",
    "                next_state, reward, done, trunc, _ = self.env.step(action)\n",
    "                \n",
    "                # Stocker la transition dans le buffer\n",
    "                transition = (state, action, reward, done, next_state)  # Utiliser un tuple\n",
    "                self.buffer.append(transition)\n",
    "\n",
    "                # Effectuer l'entraînement après avoir stocké la transition\n",
    "                for _ in range(self.nb_gradient_steps):\n",
    "                    self.gradient_step(t)\n",
    "                \n",
    "                # Réinitialiser l'état si l'épisode est terminé\n",
    "                if done or trunc:\n",
    "                    state = self.env.reset()[0]  # Réinitialisation de l'environnement\n",
    "                    self.episode += 1\n",
    "                else:\n",
    "                    state = next_state  # Continuer avec l'état suivant\n",
    "\n",
    "                self.s = state.copy()  # Mettre à jour l'état actuel\n",
    "\n",
    "            # Enregistrer la politique après l'entraînement\n",
    "            self.PI = self.definePI()\n",
    "\n",
    "            # Sauvegarder le modèle à la fin de l'entraînement\n",
    "            self.save(path)\n",
    "\n",
    "    def definePI(self):\n",
    "        def pi_function(state):\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            action = q_values.argmax(1).item()\n",
    "            return action\n",
    "        return pi_function\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            # Exploration : choisir une action aléatoire\n",
    "            return np.random.choice(self.n_actions)\n",
    "        else:\n",
    "            # Exploitation : choisir l'action avec la plus grande Q-value\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_network(state)\n",
    "            action = q_values.argmax(1).item()\n",
    "            return action\n",
    "        \n",
    "    def greedy_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        q_values = self.q_network(state)\n",
    "        action = q_values.argmax(1).item()\n",
    "        return action\n",
    "\n",
    "    def gradient_step(self, t):\n",
    "        # Calculer la perte\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return  # Pas assez de transitions pour faire un gradient step\n",
    "\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        states, actions, rewards, dones, next_states = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Calculer les Q-values pour l'état actuel\n",
    "        q_values = self.q_network(states)\n",
    "        q_value = q_values.gather(1, actions.unsqueeze(1))\n",
    "\n",
    "        # Calculer les Q-values pour l'état suivant avec le réseau cible\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states)\n",
    "            next_q_value = next_q_values.max(1)[0]\n",
    "            target_q_value = rewards + (1 - dones) * self.gamma * next_q_value\n",
    "\n",
    "        # Calculer la perte et faire un pas de gradient\n",
    "        loss = self.criterion(q_value.squeeze(1), target_q_value)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Mise à jour \"soft\" du réseau cible\n",
    "        self.soft_update()\n",
    "\n",
    "    def soft_update(self):\n",
    "        for target_param, param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_((1.0 - self.tau) * target_param.data + self.tau * param.data)\n",
    "\n",
    "    def reset(self):\n",
    "        if not hasattr(self, 'epsilon'):\n",
    "            self.epsilon = self.eps\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        self.env.reset()\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.q_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'episode': self.episode\n",
    "        }, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "        self.q_network.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.episode = checkpoint['episode']\n",
    "\n",
    "    def seed(self, seed_value):\n",
    "        random.seed(seed_value)\n",
    "        np.random.seed(seed_value)\n",
    "        torch.manual_seed(seed_value)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "agent = DQNAgent()\n",
    "agent.trainDQN(path=\"models/DQN_trained.pth\")\n",
    "agent.load(filename=\"models/DQN_trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Évaluation de l'agent sur 4 champs de vent initiaux...\n",
      "\n",
      "Champ de vent initial : simple_static\n",
      "  Taux de succès : 0.00%\n",
      "  Récompense moyenne : 0.00\n",
      "  Nombre moyen de pas : 200.0\n",
      "\n",
      "Champ de vent initial : training_1\n",
      "  Taux de succès : 0.00%\n",
      "  Récompense moyenne : 0.00\n",
      "  Nombre moyen de pas : 200.0\n",
      "\n",
      "Champ de vent initial : training_2\n",
      "  Taux de succès : 0.00%\n",
      "  Récompense moyenne : 0.00\n",
      "  Nombre moyen de pas : 200.0\n",
      "\n",
      "Champ de vent initial : training_3\n",
      "  Taux de succès : 0.00%\n",
      "  Récompense moyenne : 0.00\n",
      "  Nombre moyen de pas : 200.0\n",
      "\n",
      "==================================================\n",
      "Taux de succès GLOBAL : 0.00%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Choisir les champs de vent de formation à évaluer\n",
    "TRAINING_INITIAL_WINDFIELDS = [\"simple_static\", \"training_1\", \"training_2\", \"training_3\"]\n",
    "\n",
    "# Paramètres d'évaluation pour tous les champs de vent initiaux\n",
    "ALL_SEEDS = [42, 43, 44, 45, 46]  # Graines pour les évaluations\n",
    "ALL_MAX_HORIZON = 200             # Nombre maximal de pas par épisode\n",
    "\n",
    "# S'assurer que l'agent a été chargé avant d'exécuter\n",
    "if 'agent' in locals():\n",
    "    # Stocker les résultats pour chaque champ de vent initial\n",
    "    all_results = {}\n",
    "    \n",
    "    print(f\"Évaluation de l'agent sur {len(TRAINING_INITIAL_WINDFIELDS)} champs de vent initiaux...\")\n",
    "    \n",
    "    # Évaluer sur chaque champ de vent initial\n",
    "    for initial_windfield_name in TRAINING_INITIAL_WINDFIELDS:\n",
    "        print(f\"\\nChamp de vent initial : {initial_windfield_name}\")\n",
    "        \n",
    "        # Obtenir le champ de vent initial\n",
    "        initial_windfield = get_initial_windfield(initial_windfield_name)\n",
    "        \n",
    "        # Exécuter l'évaluation\n",
    "        results = evaluate_agent(\n",
    "            agent=agent,  # Agent DQN chargé\n",
    "            initial_windfield=initial_windfield,\n",
    "            seeds=ALL_SEEDS,\n",
    "            max_horizon=ALL_MAX_HORIZON,\n",
    "            verbose=False,  # Moins verbeux pour plusieurs évaluations\n",
    "            render=False,\n",
    "            full_trajectory=False\n",
    "        )\n",
    "        \n",
    "        # Stocker les résultats\n",
    "        all_results[initial_windfield_name] = results\n",
    "        \n",
    "        # Afficher le résumé des résultats pour ce champ de vent\n",
    "        print(f\"  Taux de succès : {results['success_rate']:.2%}\")\n",
    "        print(f\"  Récompense moyenne : {results['mean_reward']:.2f}\")\n",
    "        print(f\"  Nombre moyen de pas : {results['mean_steps']:.1f}\")\n",
    "    \n",
    "    # Afficher la performance globale\n",
    "    total_success = sum(r['success_rate'] for r in all_results.values()) / len(all_results)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Taux de succès GLOBAL : {total_success:.2%}\")\n",
    "    print(\"=\"*50)\n",
    "else:\n",
    "    print(\"Agent non chargé. Assurez-vous d'avoir chargé l'agent avant d'exécuter l'évaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sailing-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
