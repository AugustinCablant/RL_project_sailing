{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "import xgboost as xgb\n",
    "import matplotlib.animation as animation\n",
    "from typing import Dict, Any\n",
    "from copy import deepcopy\n",
    "import random \n",
    "from sklearn.base import BaseEstimator\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.distributions.categorical import Categorical\n",
    "from sklearn.utils import gen_batches\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import the BaseAgent class\n",
    "from src.agents.base_agent import BaseAgent\n",
    "from initial_windfields import get_initial_windfield, INITIAL_WINDFIELDS\n",
    "from src.env_sailing import SailingEnv\n",
    "from src.test_agent_validity import validate_agent, load_agent_class\n",
    "from src.evaluation import evaluate_agent, visualize_trajectory\n",
    "\n",
    "# Environment parameters\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "n_actions = env.action_space.n\n",
    "d_s = 2054"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the playing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_animation(imgs):\n",
    "  \"\"\"\n",
    "  Makes an animation from a list of images\n",
    "  Parameters\n",
    "  ----------\n",
    "  imgs: list of (height, width, 3) np arrays\n",
    "    list of images\n",
    "  Return\n",
    "  -------\n",
    "  ani: animation\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots()\n",
    "  draw = []\n",
    "  for i in range(len(imgs)):\n",
    "    draw_i = ax.imshow(imgs[i])\n",
    "    if i == 0:\n",
    "      ax.imshow(imgs[0]) # Show an initial one first\n",
    "    draw.append([draw_i])\n",
    "  plt.close()\n",
    "  ani = animation.ArtistAnimation(fig, draw, interval=200, blit=True,\n",
    "                              repeat=False)\n",
    "  return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_policy(env, pi, horizon=200, capture_rate=1):\n",
    "  s, _ = env.reset()\n",
    "  a = pi(s)\n",
    "  imgs = []\n",
    "  imgs.append(env.render())\n",
    "  for tt in range(horizon):\n",
    "    s, rew, term, trunc, _ = env.step(a)\n",
    "    a = pi(s)\n",
    "    if tt % capture_rate == 0:\n",
    "      imgs.append(env.render())\n",
    "    if term or trunc:\n",
    "      break\n",
    "  return make_animation(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_1\n",
      "{'wind_init_params': {'base_speed': 3.0, 'base_direction': (-0.8, -0.2), 'pattern_scale': 32, 'pattern_strength': 0.3, 'strength_variation': 0.4, 'noise': 0.1}, 'wind_evol_params': {'wind_change_prob': 1.0, 'pattern_scale': 128, 'perturbation_angle_amplitude': 0.1, 'perturbation_strength_amplitude': 0.1, 'rotation_bias': 0.02, 'bias_strength': 1.0}, 'env_params': {'wind_grid_density': 25, 'wind_arrow_scale': 80, 'render_mode': 'rgb_array'}}\n",
      "training_2\n",
      "{'wind_init_params': {'base_speed': 3.0, 'base_direction': (-0.2, 0.8), 'pattern_scale': 128, 'pattern_strength': 0.6, 'strength_variation': 0.3, 'noise': 0.1}, 'wind_evol_params': {'wind_change_prob': 1.0, 'pattern_scale': 128, 'perturbation_angle_amplitude': 0.1, 'perturbation_strength_amplitude': 0.1, 'rotation_bias': 0.02, 'bias_strength': 1.0}, 'env_params': {'wind_grid_density': 25, 'wind_arrow_scale': 80, 'render_mode': 'rgb_array'}}\n",
      "training_3\n",
      "{'wind_init_params': {'base_speed': 3.0, 'base_direction': (0.2, -0.8), 'pattern_scale': 32, 'pattern_strength': 0.4, 'strength_variation': 0.2, 'noise': 0.1}, 'wind_evol_params': {'wind_change_prob': 1.0, 'pattern_scale': 128, 'perturbation_angle_amplitude': 0.1, 'perturbation_strength_amplitude': 0.1, 'rotation_bias': 0.02, 'bias_strength': 1.0}, 'env_params': {'wind_grid_density': 25, 'wind_arrow_scale': 80, 'render_mode': 'rgb_array'}}\n",
      "simple_static\n",
      "{'wind_init_params': {'base_speed': 3.0, 'base_direction': (-0.7, -0.7), 'pattern_scale': 32, 'pattern_strength': 0.1, 'strength_variation': 0.1, 'noise': 0.05}, 'wind_evol_params': {'wind_change_prob': 0.0, 'pattern_scale': 128, 'perturbation_angle_amplitude': 0.0, 'perturbation_strength_amplitude': 0.0, 'rotation_bias': 0.0, 'bias_strength': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "for initial_windfield_name, initial_windfield in INITIAL_WINDFIELDS.items():\n",
    "    print(initial_windfield_name)\n",
    "    print(initial_windfield)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitted Q iteration\n",
    "\n",
    ">Implement fitted Q iterations with random forest using uniform exploration for $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect a dataset \n",
    "\n",
    "def collect_dataset(pi= lambda x: np.random.randint(0, n_actions) , n=10000):\n",
    "    \"\"\"\n",
    "    Collect a dataset of the form $(s_i, a_i, r_{a_i}(s_i), s'_i)_{i=1}^n$\n",
    "    by running a policy that chooses its action uniformly at random\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    env: Environment\n",
    "    pi: Policy\n",
    "    n: int\n",
    "        Number of samples to collect\n",
    "\n",
    "    Return\n",
    "    -----\n",
    "    data: np array of size (n, 2d_s + d_a + 2)\n",
    "        data collected by the random policy\n",
    "        the first 4 columns are the states s_i,\n",
    "        the 5th, 6th and 7th columns contain the actions a_i,\n",
    "        rewards r_{a_i}(s_i) and whether the step is a termination step\n",
    "        and the columns 8th-12th columns contain the states s'_i\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for initial_windfield_name, initial_windfield in INITIAL_WINDFIELDS.items():\n",
    "        env = SailingEnv(**get_initial_windfield(initial_windfield_name))\n",
    "        s0, _ = env.reset()\n",
    "        s = s0.copy()\n",
    "        n_actions = env.action_space.n\n",
    "\n",
    "        for i in range(n//4):\n",
    "            a = pi(s)\n",
    "            s2, r, done, trunc, _ = env.step(a)\n",
    "            data.append(s.copy().tolist() + [a, r, done] + s2.copy().tolist())\n",
    "            if done or trunc:\n",
    "                s, _ = env.reset()\n",
    "            else:\n",
    "                s = s2.copy()\n",
    "\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [33:55:12<00:00, 1221.13s/it]  \n"
     ]
    }
   ],
   "source": [
    "class FQI(BaseAgent):\n",
    "    \"\"\" FQI agent\"\"\"\n",
    "    \n",
    "    def __init__(self, data=collect_dataset()):\n",
    "        super().__init__()\n",
    "        self.d_s = 2054\n",
    "        self.data = data\n",
    "        self.gamma = 0.99 \n",
    "        self.n_iterations = 100\n",
    "        self.n_actions = 9\n",
    "        self.epsilon = 0.2\n",
    "        self.model = RandomForestRegressor()\n",
    "        self.pi = None\n",
    "        \n",
    "    \n",
    "    def act(self, observation: np.ndarray) -> int:\n",
    "        if self.pi is None:\n",
    "            print(\"The Agent has not been trained\")\n",
    "        else:\n",
    "            return int(self.pi(observation))\n",
    "    \n",
    "    def trainFQI(self):\n",
    "        # Use the data collected before\n",
    "        n = len(self.data)\n",
    "        states, actions, rewards, dones, next_states = self.data[:, :2054], self.data[:, 2054], self.data[:, 2054+1], self.data[:, 2054+2], self.data[:, 2054+3:]\n",
    "        X, Y = self.data[:, :2054+1], self.data[:, 2054+1]\n",
    "        self.model.fit(X, Y)\n",
    "\n",
    "        for _ in tqdm(range(self.n_iterations)):\n",
    "            Qmax = np.max(\n",
    "                            [\n",
    "                            self.model.predict(np.column_stack([\n",
    "                                            self.data[:, d_s + 3:],\n",
    "                                            np.ones(n).reshape(-1, 1) * a\n",
    "                                            ]))\n",
    "                                for a in range(n_actions)\n",
    "                            ],axis=0)\n",
    "            Y = self.data[:, d_s + 1] + self.gamma * (1 - dones) * Qmax\n",
    "            self.model.fit(X, Y)\n",
    "        \n",
    "        pi = lambda s: np.argmax(\n",
    "                                [self.model.predict(np.array(s.tolist() + [a]).reshape(1, -1))[0] for a in range(n_actions)]\n",
    "                                )\n",
    "        self.pi = pi\n",
    "        agent.save(path=\"models/FQI_RF\")\n",
    "        return pi\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the agent.\"\"\"\n",
    "        pass  # Nothing to reset in this simple agent\n",
    "    \n",
    "    def seed(self, seed: int = None) -> None:\n",
    "        \"\"\"Set the random seed.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "\n",
    "    def save(self, path):\n",
    "        if self.model is not None:\n",
    "            joblib.dump(self.model, path)\n",
    "        else:\n",
    "            print(\"No model found to save.\")\n",
    "\n",
    "    def load(self):\n",
    "        try:\n",
    "            self.model = joblib.load(\"models/FQI_RF\")\n",
    "        except:\n",
    "            print(\"No saved model found.\")\n",
    "            self.model = None\n",
    "\n",
    "\n",
    "agent = FQI()\n",
    "pi_FQI = agent.trainFQI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating agent on 4 training initial windfields...\n",
      "\n",
      "Initial windfield: simple_static\n",
      "  Success Rate: 0.00%\n",
      "  Mean Reward: 0.00\n",
      "  Mean Steps: 200.0\n",
      "\n",
      "Initial windfield: training_1\n",
      "  Success Rate: 20.00%\n",
      "  Mean Reward: 7.70\n",
      "  Mean Steps: 179.2\n",
      "\n",
      "Initial windfield: training_2\n",
      "  Success Rate: 0.00%\n",
      "  Mean Reward: 0.00\n",
      "  Mean Steps: 200.0\n",
      "\n",
      "Initial windfield: training_3\n",
      "  Success Rate: 20.00%\n",
      "  Mean Reward: 3.12\n",
      "  Mean Steps: 197.2\n",
      "\n",
      "==================================================\n",
      "OVERALL SUCCESS RATE: 10.00%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Choose which training initial windfields to evaluate on\n",
    "TRAINING_INITIAL_WINDFIELDS = [\"simple_static\", \"training_1\", \"training_2\", \"training_3\"]\n",
    "\n",
    "# Evaluation parameters for all initial windfields\n",
    "ALL_SEEDS = [42, 43, 44, 45, 46]  # Seeds to use for all evaluations\n",
    "ALL_MAX_HORIZON = 200             # Maximum steps per episode\n",
    "\n",
    "# Only run if the agent was successfully loaded\n",
    "if 'agent' in locals():\n",
    "    # Store results for each initial windfield\n",
    "    all_results = {}\n",
    "    \n",
    "    print(f\"Evaluating agent on {len(TRAINING_INITIAL_WINDFIELDS)} training initial windfields...\")\n",
    "    \n",
    "    # Evaluate on each initial windfield\n",
    "    for initial_windfield_name in TRAINING_INITIAL_WINDFIELDS:\n",
    "        print(f\"\\nInitial windfield: {initial_windfield_name}\")\n",
    "        \n",
    "        # Get the initial windfield\n",
    "        initial_windfield = get_initial_windfield(initial_windfield_name)\n",
    "        \n",
    "        # Run the evaluation\n",
    "        results = evaluate_agent(\n",
    "            agent=agent,\n",
    "            initial_windfield=initial_windfield,\n",
    "            seeds=ALL_SEEDS,\n",
    "            max_horizon=ALL_MAX_HORIZON,\n",
    "            verbose=False,  # Less verbose for multiple evaluations\n",
    "            render=False,\n",
    "            full_trajectory=False\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_results[initial_windfield_name] = results\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"  Success Rate: {results['success_rate']:.2%}\")\n",
    "        print(f\"  Mean Reward: {results['mean_reward']:.2f}\")\n",
    "        print(f\"  Mean Steps: {results['mean_steps']:.1f}\")\n",
    "    \n",
    "    # Print overall performance\n",
    "    total_success = sum(r['success_rate'] for r in all_results.values()) / len(all_results)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"OVERALL SUCCESS RATE: {total_success:.2%}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  Define the Neural network with the __init__ and forward method.\n",
    "  It should define a fully connected\n",
    "  neural network with prescribed input size, hidden size and output size\n",
    "  \"\"\"\n",
    "  def __init__(self, input_size, hidden_size, output_size) -> None:\n",
    "    super().__init__()\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "            )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.linear_relu_stack(x)\n",
    "\n",
    "\n",
    "class NN(nn.Module):\n",
    "  \"\"\"\n",
    "  A sklearn-like class for the neural net\n",
    "  \"\"\"\n",
    "  def __init__(self, n_iterations, input_size, hidden_size, output_size, alpha, seed, batch_size) -> None:\n",
    "    \"\"\"\n",
    "    Initialize the sklearn class:\n",
    "    - Record the input parameters using\n",
    "    self.parameter = parameter\n",
    "    - Initialize the neural network model and record it in self.model\n",
    "    - Initialize the Adam optimizer and record it in self.optimizer\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "    self.n_iterations = n_iterations\n",
    "    self.alpha = alpha\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.seed = seed\n",
    "    self.batch_size = batch_size\n",
    "    self.model = FCNet(input_size, hidden_size, output_size).to(self.device)\n",
    "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=alpha)\n",
    "\n",
    "\n",
    "  def partial_fit(self, X, Y):\n",
    "    \"\"\"Update parameters\n",
    "    - Convert numpy input data to torch\n",
    "    - Define the loss\n",
    "    - Find the gradient via automatic differentiation\n",
    "    - Perform a gradient update of the parameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np array of size (n, ds + da)\n",
    "      state-action pairs\n",
    "    Y: np array of size (n, 1)\n",
    "      Estimate of the state action value function\n",
    "    \"\"\"\n",
    "    # DATA\n",
    "    Xs = torch.from_numpy(X[:, :-1].copy()).float().to(self.device)  # états\n",
    "    Xa = torch.from_numpy(X[:, -1].copy()).long().to(self.device)    # actions (indices)\n",
    "    if isinstance(Y, np.ndarray):\n",
    "        Y = torch.from_numpy(Y.copy()).float().to(self.device)\n",
    "    else:\n",
    "        Y = Y.float().to(self.device)\n",
    "\n",
    "    # LOSS\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    Q_all = self.model(Xs)\n",
    "    Q_pred = Q_all[torch.arange(len(Xs)), Xa]\n",
    "    loss = loss_fn(Q_pred, Y)\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "  def fit(self, X, Y):\n",
    "    \"\"\"Applies n_iterations steps of gradient using function grad_step.\n",
    "    At each iteration, all samples are shuffled and split into batches of size\n",
    "    `batch_size`.\n",
    "    The gradient steps are performed on each batch of data sequentially\n",
    "    \"\"\"\n",
    "    idx_samples = np.arange(len(X))\n",
    "    for _ in range(self.n_iterations):\n",
    "      np.random.shuffle(idx_samples)\n",
    "      for batch_slice in gen_batches(len(X), self.batch_size):\n",
    "        Xb, Yb = X[idx_samples[batch_slice]].copy(), Y[idx_samples[batch_slice]].copy()\n",
    "        self.partial_fit(Xb, Yb)\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"Use the fitted parameter to predict q(s, a)\n",
    "    - Convert input data to Torch\n",
    "    - Apply the model\n",
    "    - Convert the output to numpy\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    X: np array of shape (n, ds + da)\n",
    "    Return\n",
    "    ------\n",
    "    qSA: np array of shape (n,)\n",
    "      qSA[i] is an estimate of q(s_i, a_i) computed with the pred function\n",
    "      where s_i and a_i are given by X[i]\n",
    "    \"\"\"\n",
    "    Xs = X[:, :-1]\n",
    "    Xa = X[:, -1]\n",
    "    with torch.no_grad():\n",
    "      Xs = torch.from_numpy(Xs.copy()).float().to(self.device)\n",
    "      Xa = torch.from_numpy(Xa.copy()).int().to(self.device)\n",
    "      y = self.model(Xs)[torch.arange(len(Xs)), Xa]\n",
    "    return y.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 426/1000 [00:38<00:49, 11.61it/s]"
     ]
    }
   ],
   "source": [
    "class DQNAgent(BaseAgent):\n",
    "    \"\"\" DQN Agent \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "        self.d_s = 2054\n",
    "        self.n_actions = 9\n",
    "        self.pi = lambda x: np.random.randint(0, self.n_actions)\n",
    "        self.capacity = 2000\n",
    "        self.batch_size = 200\n",
    "        self.eps = 0.5\n",
    "        self.gamma = 0.99\n",
    "        self.C = 20\n",
    "        self.n_iterations = 1000\n",
    "        self.nb_gradient_steps = 5  # Number of gradient steps after each iteration\n",
    "        self.model = NN(n_iterations=1,\n",
    "                        input_size=2054,\n",
    "                        hidden_size=24,\n",
    "                        output_size=env.action_space.n,\n",
    "                        alpha=0.001,\n",
    "                        batch_size=None,\n",
    "                        seed=0)\n",
    "        self.buffer = np.zeros((self.capacity, self.d_s * 2 + 3))\n",
    "        self.target_model = deepcopy(self.model)\n",
    "        self.Qmax = None \n",
    "        self.memory = []\n",
    "        self.criterion = torch.nn.SmoothL1Loss()\n",
    "        self.reset()\n",
    "\n",
    "    def sample(self):\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        return list(map(lambda x:torch.Tensor(np.array(x)).to(self.device), list(zip(*batch))))\n",
    "\n",
    "    def gradient_step(self, t):\n",
    "        if len(self.buffer) > self.batch_size:\n",
    "            I = np.random.randint(min(t+1, self.capacity), size=self.batch_size)\n",
    "            data = self.buffer[I]\n",
    "\n",
    "            # update target network if needed\n",
    "            if t % self.C == 0:\n",
    "                self.target_model = deepcopy(self.model)\n",
    "\n",
    "            Xb = data[:, :len(self.s) + 1]\n",
    "            Yb = data[:, len(self.s) + 1]\n",
    "\n",
    "            # Qmax update\n",
    "            if self.Qmax is None:\n",
    "                self.model.partial_fit(Xb, Yb)\n",
    "\n",
    "            self.Qmax = np.max(\n",
    "                            [self.target_model.predict(np.column_stack([data[:, len(self.s) + 3:], np.ones(len(data)).reshape(-1, 1) * a])) for a in range(n_actions)],\n",
    "                            axis=0)\n",
    "            Yb = data[:, len(self.s) + 1] + self.gamma * (1 - data[:, len(self.s) + 2]) * self.Qmax\n",
    "            Yb = torch.from_numpy(Yb.copy()).to(torch.float).to(self.model.device)\n",
    "            self.model.partial_fit(Xb, Yb)\n",
    "\n",
    "    def greedy_action(self, state):\n",
    "        return np.random.randint(0, self.n_actions)\n",
    "    \n",
    "    def definePI(self):\n",
    "        def pi(s):\n",
    "            q = [self.model.predict(np.array(self.s.copy().tolist() + [a]).reshape(1,-1))[0] for a in range(self.n_actions)]\n",
    "            return np.argmax(q)\n",
    "        return lambda s: pi(s)\n",
    "\n",
    "    def trainDQN(self, path):\n",
    "        for initial_windfield_name, initial_windfield in INITIAL_WINDFIELDS.items():\n",
    "            env = SailingEnv(**get_initial_windfield(initial_windfield_name))\n",
    "            state, _ = env.reset()\n",
    "            epsilon = self.eps\n",
    "            self.s = state.copy()\n",
    "\n",
    "            for t in tqdm(range(self.n_iterations)):\n",
    "                # select epsilon-greedy action\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = self.greedy_action(state)\n",
    "                # step\n",
    "                next_state, reward, done, trunc, _ = env.step(action)\n",
    "                self.buffer[t % self.capacity, :] = np.array(state.copy().tolist() + [action, reward, done] + next_state.copy().tolist()) # [s, a, r, term, s2]\n",
    "\n",
    "                # train\n",
    "                for _ in range(self.nb_gradient_steps):   \n",
    "                    self.gradient_step(t)\n",
    "\n",
    "                if done or trunc:\n",
    "                    state, _ = env.reset()\n",
    "                    self.s = state.copy()\n",
    "                else:\n",
    "                    state = next_state\n",
    "                    self.s = next_state.copy()\n",
    "        self.save(path)\n",
    "    \n",
    "    def act(self, observation: np.ndarray) -> int:\n",
    "        \"\"\" \"\"\"\n",
    "        with torch.no_grad():\n",
    "            Q = self.model(torch.Tensor(observation).unsqueeze(0).to(self.device))\n",
    "            probs = Categorical(Q)\n",
    "            action = probs.sample()\n",
    "        return action\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the agent.\"\"\"\n",
    "        self.s0, _ = env.reset()\n",
    "        self.s = self.s0.copy()\n",
    "    \n",
    "    def save(self, path):\n",
    "        if self.model is not None:\n",
    "            torch.save(self.model.state_dict(), path)\n",
    "        else:\n",
    "            print(\"No model found to save.\")\n",
    "\n",
    "    def load(self):\n",
    "        try:\n",
    "            self.model = NN(n_iterations=1,\n",
    "                        input_size=2054,\n",
    "                        hidden_size=24,\n",
    "                        output_size=env.action_space.n,\n",
    "                        alpha=0.001,\n",
    "                        batch_size=None,\n",
    "                        seed=0).to(self.device)\n",
    "            self.model.load_state_dict(torch.load(\"models/DQN\", map_location = self.device))\n",
    "            self.model.eval()\n",
    "        except:\n",
    "            print(\"No saved model found.\")\n",
    "            self.model = None\n",
    "    \n",
    "    def seed(self, seed: int = None) -> None:\n",
    "        \"\"\"Set the random seed.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "\n",
    "agentDQN = DQNAgent()\n",
    "agentDQN.trainDQN(path=\"models/DQN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentDQN.PI(env.reset()[0])  #example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd '/Users/augustincablant/Documents/GitHub/RL_project_sailing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mError: File 'RL_project_sailing/notebooks/agents/DQN.py' not found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python /Users/augustincablant/Documents/GitHub/RL_project_sailing/src/test_agent_validity.py RL_project_sailing/notebooks/agents/DQN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating agent on 4 training initial windfields...\n",
      "\n",
      "Initial windfield: simple_static\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NN' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m initial_windfield = get_initial_windfield(initial_windfield_name)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Run the evaluation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m results = \u001b[43mevaluate_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[43magentDQN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_windfield\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_windfield\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mALL_SEEDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_horizon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mALL_MAX_HORIZON\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Less verbose for multiple evaluations\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_trajectory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[32m     34\u001b[39m all_results[initial_windfield_name] = results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/RL_project_sailing/src/evaluation.py:92\u001b[39m, in \u001b[36mevaluate_agent\u001b[39m\u001b[34m(agent, initial_windfield, seeds, max_horizon, verbose, render, full_trajectory)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Run episode\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m steps < max_horizon:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Get action from agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     action = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     episode_actions.append(action)\n\u001b[32m     95\u001b[39m     \u001b[38;5;66;03m# Step environment\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mDQNAgent.act\u001b[39m\u001b[34m(self, observation)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" \"\"\"\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     Q = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     probs = Categorical(Q)\n\u001b[32m    103\u001b[39m     action = probs.sample()\n",
      "\u001b[31mTypeError\u001b[39m: 'NN' object is not callable"
     ]
    }
   ],
   "source": [
    "# Choose which training initial windfields to evaluate on\n",
    "TRAINING_INITIAL_WINDFIELDS = [\"simple_static\", \"training_1\", \"training_2\", \"training_3\"]\n",
    "\n",
    "# Evaluation parameters for all initial windfields\n",
    "ALL_SEEDS = [42, 43, 44, 45, 46]  # Seeds to use for all evaluations\n",
    "ALL_MAX_HORIZON = 200             # Maximum steps per episode\n",
    "\n",
    "# Only run if the agent was successfully loaded\n",
    "if 'agent' in locals():\n",
    "    # Store results for each initial windfield\n",
    "    all_results = {}\n",
    "    \n",
    "    print(f\"Evaluating agent on {len(TRAINING_INITIAL_WINDFIELDS)} training initial windfields...\")\n",
    "    \n",
    "    # Evaluate on each initial windfield\n",
    "    for initial_windfield_name in TRAINING_INITIAL_WINDFIELDS:\n",
    "        print(f\"\\nInitial windfield: {initial_windfield_name}\")\n",
    "        \n",
    "        # Get the initial windfield\n",
    "        initial_windfield = get_initial_windfield(initial_windfield_name)\n",
    "        \n",
    "        # Run the evaluation\n",
    "        results = evaluate_agent(\n",
    "            agent=agentDQN,\n",
    "            initial_windfield=initial_windfield,\n",
    "            seeds=ALL_SEEDS,\n",
    "            max_horizon=ALL_MAX_HORIZON,\n",
    "            verbose=False,  # Less verbose for multiple evaluations\n",
    "            render=False,\n",
    "            full_trajectory=False\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_results[initial_windfield_name] = results\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"  Success Rate: {results['success_rate']:.2%}\")\n",
    "        print(f\"  Mean Reward: {results['mean_reward']:.2f}\")\n",
    "        print(f\"  Mean Steps: {results['mean_steps']:.1f}\")\n",
    "    \n",
    "    # Print overall performance\n",
    "    total_success = sum(r['success_rate'] for r in all_results.values()) / len(all_results)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"OVERALL SUCCESS RATE: {total_success:.2%}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sailing-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
